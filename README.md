# NI Test Data Processing Pipeline Design

## Overview

This document outlines the design for a robust and scalable data pipeline to handle high-volume test data generated by NI (National Instruments) test equipment. The system is designed to ingest a stream of thousands of floats per hour, process this data in near real-time, and persist the results for long-term analysis and reporting.

## System Architecture

The proposed architecture follows a multi-stage approach to efficiently manage the data flow from ingestion to final storage:

1.  **Data Ingestion (Streaming Layer)**: A high-throughput streaming platform (e.g., Redis Streams, Kafka) will receive the raw float data directly from the NI equipment. This layer is designed to handle high-volume, continuous data streams without loss.
2.  **Intermediate Processing & Storage (NoSQL Layer)**: A fast NoSQL database (e.g., Redis as a Document or Vector store) will consume from the stream. This layer is responsible for initial data parsing, real-time analysis (e.g., anomaly detection), and temporary storage before aggregation.
3.  **Reliable Persistence (Relational Layer)**: A MySQL database will serve as the final, reliable data store. It will house the processed, aggregated, and summarized test results, optimized for complex querying, reporting, and long-term historical analysis.

## Data Structures

### Stream Message Format

To optimize network traffic and processing efficiency, data is sent from the source in batches. The following JSON structure is proposed for each message in the data stream. This format reduces data redundancy by defining common metadata once per batch and using time offsets for individual points.

```json
{
  "session_id": "sess_20250629T143510_SN12345",
  "equipment_sn": "NI-PXI-12345",
  "batch_start_utc": "2025-06-29T14:36:00.000000Z",
  "measurement_info": {
    "name": "voltage",
    "unit": "V",
    "channel": "A"
  },
  "metadata": {
    "test_profile": "power_on_stability_test",
    "start_sequence_num": 16000,
    "count": 100
  },
  "points": [
    { "offset_ms": 0, "value": 4.987 },
    { "offset_ms": 10, "value": 4.988 },
    { "offset_ms": 990, "value": 4.986 }
  ]
}
```

## Final Database Schema

The MySQL database is designed to store processed results, not the raw high-frequency data. This ensures the database remains performant and easy to query. The schema is normalized for data integrity and includes indexes for efficient lookups.

### Key Design Decisions

*   **No Raw Floats**: The raw stream of thousands of floats is not stored in MySQL. Instead, only aggregated summaries and key events are persisted to maintain performance.
*   **Normalization**: Metric and event types are stored in lookup tables (`metric_definitions`, `event_definitions`) to enforce consistency and improve query performance over storing long strings.
*   **Auditability**: A dedicated `raw_stream_batches` table stores the exact, unprocessed JSON payload from the stream. This provides an immutable audit trail and allows for data reprocessing if needed.
*   **Flexibility**: `JSON` columns are used for storing structured metadata (`session_summary_metrics`, `detected_events`) and complex array results (`session_array_results`), providing flexibility without requiring schema changes.

### Database Schema 

<iframe width="560" height="315" src='https://dbdiagram.io/e/6862dd2ef413ba3508959c30/6862ee0ff413ba350897b2a3'> </iframe>

The complete and up-to-date database schema is defined in the `schema.sql` file. This file serves as the single source of truth for the database structure, ensuring that documentation does not become stale.